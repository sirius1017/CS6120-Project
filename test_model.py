import os
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch
from SUPPORTED_MODELS import SUPPORTED_MODELS 


def ensure_model(model_key="tinyllama"):
    """ç¡®ä¿æ¨¡å‹å­˜åœ¨æœ¬åœ°å¹¶åŠ è½½"""
    if model_key not in SUPPORTED_MODELS:
        raise ValueError(f"Unsupported model '{model_key}'. Choose from {list(SUPPORTED_MODELS.keys())}.")

    model_info = SUPPORTED_MODELS[model_key]
    hf_id = model_info["hf_id"]
    cache_dir = model_info["cache_dir"]

    print(f"ğŸ” Checking for local model: {model_key} ...")
    
    # å¦‚æœç¼“å­˜ç›®å½•ä¸å­˜åœ¨ï¼Œåˆ™è§¦å‘ä¸‹è½½
    if not os.path.exists(cache_dir) or not os.listdir(cache_dir):
        print("â¬‡ï¸ Downloading model from Hugging Face...")
    
    tokenizer = AutoTokenizer.from_pretrained(hf_id, cache_dir=cache_dir)
    model = AutoModelForCausalLM.from_pretrained(
        hf_id,
        cache_dir=cache_dir,
        torch_dtype=torch.float32
    )

    device = 0 if torch.cuda.is_available() else -1
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        device=device,
        max_new_tokens=150,
        do_sample=True,
        temperature=0.7,
    )

    print(f"âœ… Model '{model_key}' ready to use (device: {'cuda' if device == 0 else 'cpu'})")
    return pipe

def generate_response(model_pipe, user_input):
    """ä½¿ç”¨æ¨¡å‹ç”Ÿæˆå›åº”"""
    prompt = f"""You are a helpful cooking assistant.

Question: {user_input}
Answer:"""
    output = model_pipe(prompt)[0]["generated_text"]
    return output.replace(prompt, "").strip()


if __name__ == "__main__":
    # model_name = "tinyllama"
    model_key = "deepseek"

    pipe = ensure_model(model_key)
    print(f'Using model {SUPPORTED_MODELS[model_key]["hf_id"]}')
    
    while True:
        print("ğŸ§‘â€ğŸ³ No-RAG chef is ready. Type your cooking question!\n(Type 'exit' to quit)")
        print("ğŸ¤” You: ")
        user_input = input()
        if user_input.lower() in {"exit"}:
            break
        print("ğŸ¤– Generating...\n")
        response = generate_response(pipe, user_input)
        print("ğŸ½ï¸ Answer:", response)


